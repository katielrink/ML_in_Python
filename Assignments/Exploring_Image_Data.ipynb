{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Two: Exploring Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a3058b283ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Importing Tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import re\n",
    "import plotly\n",
    "from plotly.graph_objs import Bar, Line\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "from plotly.graph_objs.scatter import Marker\n",
    "from plotly.graph_objs.layout import XAxis, YAxis\n",
    "from sklearn.decomposition import PCA\n",
    "import os, random, shutil\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the production of legos, millions of legos are created on the daily. It is a necessary task to be able to recogonize the characteristics of different bricks and sort them accordingly. In the below dataset, there are 40,000 digitally recreated images of 50 different lego pieces at 800 different angles. This dataset was originally designed for research and learning purposes, and it was designed to train an algorithm to categorize different lego pieces based on appearance. The goal is to be able to predict what kind of lego brick is in a photo with a 98% accuracy. If the model were to achieve any lower levels of accuracy, the amount of displaced legos would be in the hundred thousands daily. Generally, 2% is considered standard error to be accepted. \n",
    "\n",
    "**Data Set :**\n",
    "Images of Lego Brick - *https://www.kaggle.com/datasets/joosthazelzet/lego-brick-images*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Information about Images being Read In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information is saved within a single folder full of pngs. The title of each png is it's ID, the name of the piece, and then the size. To gather all the information about the pngs, we must first save all the urls, so that they can all be loaded into our program. Then we need to seperate the name and size from the path, so that we can label each of our images.</br>\n",
    "Due to restrictions in computing power, and my computer's constant tendancy to run out of RAM to store arrays, I only took the 2667 images. More images than that caused my processor to be unable to complete the necessary calculations. Since there were 800 images for each brick type, taking any consequetive images would cause us to only be able to identify two or three types of bricks. As the shapes are quite simple, it should not take 800 images to train an algorithm to recognize it. Additionally, the recognizition of only a select few types would lend very little help in sorting legos. Resultantly, I chose to take a every 15th image, which allows us to have an even distribution of all the bricks and consistency among the angles. Roughly, we should end up with 53 images per brick</br>\n",
    "I also chose not to include the ID of the lego brick in the name, as it cluttered our later images and did not carry much necessary meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referenced Datalira at https://www.kaggle.com/databeru\n",
    "#Create a list with the filepaths\n",
    "dir = Path('C:/Users/katie/Documents/GitHub/Data_Science/ML_in_Python/Data/Lego_Images/test/dataset')\n",
    "    \n",
    "file_paths = list(dir.glob(r'**/*.png'))\n",
    "\n",
    "df = [str(x) for x in file_paths[::15]]\n",
    "df = pd.DataFrame({'Filepath':file_paths[::15]})\n",
    "\n",
    "#Add Labels from path\n",
    "def get_label(string):\n",
    "    #Isolate name from path\n",
    "    label  = ' '.join(string.split('/')[-1].replace('.png', '').split(' ')[1:-1])\n",
    "    label = label.lower()\n",
    "    #remove initial id value\n",
    "    label = label[re.search(r'[a-z]', label).start():]\n",
    "    return label\n",
    "           \n",
    "# Retrieve the label from the path of the pictures\n",
    "df['Label'] = df['Filepath'].apply(lambda x: get_label(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Convert Images to Greyscale and Lower Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce our images from several channels to 1, I chose to read in the images in greyscale. This allowed for each in later linearizing the data into a 1D array, which would have been impossible with more than 1 channels. The CYMK colors did not provide any use in recognizing the features of the bricks, and having the colors would make the process noisier and more difficult</br>\n",
    "I then chose to remove unnecessary computational time by cropping the images of much of the white space surrounding the borders. Via trial and error, I found that there were roughly 90 additional pixels on every edge of just white space. Because this didn't aid in our feature recognizition and was taking up space that I quite frankly did not have, I simply cropped it out. </br>\n",
    "As the features on each images are quite large, the details of each image do not need to be very precise to maintain accuracy. So to again save the space needed to process each image, I lowered the quality to roughly 1/2 of it's original resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in df['Filepath']: \n",
    "    #Open file in grayscale\n",
    "    img = Image.open(url).convert('L')\n",
    "    #Crop image down\n",
    "    img = img.crop((90, 90, 310, 310))\n",
    "    #save at a lower resolution\n",
    "    img.save(url, quality=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Read in Images as Numpy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the images have been converted to a form that my processor can handle, I then converted all of the images into a numpy array for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert images to a numpy arrays\n",
    "original_images = df['Filepath'].apply(lambda x: np.asarray(Image.open(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2,667 images, with a shape of (220, 220). From the shape, we can tell that the photos are all 220x220 pixels. Therefore in our matrix, the rows would represent the number of images, so there are 2667 rows. And the columns are going to be the number of pixels so there are 484000 columns. Because we are in grayscale, each pixel does correlate with only with one column. We can also tell that they all have 1 channels because it does not appear in the shape, which means they range from white at 255 to black at 0. There are 46 unique labels, and therefore 46 types of bricks within this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print information about dataset\n",
    "print(len(original_images))\n",
    "print(original_images[0].shape)\n",
    "print(len(df['Label'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearize the Images to Create a Table of 1-D Image Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do further analysis on the images, I flattened my 2D images into 1D arrays. To do this, I first created an array with the number of rows equal to the number of images and the number of columns equal to the number of pixels in each images. I then filled them with zeros as a base. Following this creation, I read in each image, flattened it to 1D, and then stored it in my newly created array. </br>\n",
    "Then I converted my 2D array of all my 1D images into a dataframe, so that I can continue to run pandas functions on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array with there are 2000 rows and 48400 columns\n",
    "img_data = np.zeros((2667, 48400))\n",
    "\n",
    "for i in range(len(original_images)):\n",
    "    img = original_images[i]\n",
    "    farr = img.flatten()\n",
    "    img_data[i] = farr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert array to a dataframe\n",
    "flat_images = pd.DataFrame(img_data)\n",
    "\n",
    "flat_images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualize Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all my images are loaded and ready to analyze, it was important to view the images and ensure they still looked as I expected them to. As I was going to frequently need to view the images I was analayzing, I utilized Dr. Larson's code to create a gallery of images.</br>\n",
    "I then printed the first 18 images, in which I saw what I expected. They were black and white images of the lego blocks. All of the block was still shown, and it was still highly recognizable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference from https://github.com/eclarson/MachineLearningNotebooks/blob/master/04.%20Dimension%20Reduction%20and%20Images.ipynb\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "\n",
    "#Images : df of images in a numpy array, titles : labels of what each image shows, h : height in pixels, w : width in pixels\n",
    "#n_rows : number of rows of images (auto 3), n_col : number of images per row (auto 6)\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=6):\n",
    "    plt.figure(figsize=(1.7 * n_col, 2.3 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i+1)\n",
    "        try:\n",
    "            plt.imshow(images.iloc[i].values.reshape(h,w), cmap=plt.cm.gray)\n",
    "        except:\n",
    "            plt.imshow(images[i].reshape(h,w), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gallery(flat_images, df['Label'], 220, 220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Linear dimensionality reduction --- Full PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to derive new features from the images we have, we need to use dimensional analysis. Our first method is a full Principal Component Analysis, which helps us capture the largest amount of variation in data. In order to understand the information we are going to work with, we need to gather the samples, features, classes, and size of the data. The samples is the number of images we have to analyze. The features is the number of different kinds of bricks there are. The classes are the number of labels. h is the height of the images in pixels, and w is the width of images in pixels. </br>\n",
    "We also printed out a sample of the images, just as a reference of what we're looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = flat_images\n",
    "y = df['Label']\n",
    "\n",
    "target_names = df['Label']\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "h, w = original_images[0].shape\n",
    "n_classes = len(target_names)\n",
    "\n",
    "print(\"n_samples: {}\".format(n_samples))\n",
    "print(\"n_features: {}\".format(n_features))\n",
    "print(\"n_classes: {}\".format(n_classes))\n",
    "print(\"Original Image Sizes {} by {}\".format(h,w))\n",
    "#Print Information about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_gallery(X, target_names, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our information, we can move forward with calculating our PCA. We pull out all the components, which we choose to be the number of samples we have because are doing a complete analysis. The PCA is calculated for us by finding the eigenvectors and then the covariance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Full PCA \n",
    "n_components = 2667\n",
    "print (\"Extracting the top %d eigenpieces from %d lego faces\" % (n_components, X.shape[0]))\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X.copy())\n",
    "eigenpieces = pca.components_.reshape((n_components, h, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each principlate component shows you a certain amount of variation in the data which is encoded in the eigenvalue for each eigenvector. It is found through finding the factors of the eigen values. It can be calculated via the formula below.</br>\n",
    "The plot that is formed shows that most of the variance can be explained by the principal component at around 30% variance. The 2-5 components also adds roughly about 30% of the variance. From there the remaining components, the remaining 40% are accumulated, and they trail off exponentially. When interacting with below plot, we can see that we reach 90% explained variance with roughly 100 components. 100 out of over 2000 components is very low, which means we can reach a very high fidelity reconstruction with very few components</br>\n",
    "Ultimately, we have determined that we can gather a more clear reconstruction with our full dataset, even though the first couple images covers most of our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$ r_q=\\frac{\\sum_{j=1}^q \\lambda_j}{\\sum_{\\forall i} \\lambda_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Explained Variance for Each Component\n",
    "#Reference from https://github.com/eclarson/MachineLearningNotebooks/blob/master/04.%20Dimension%20Reduction%20and%20Images.ipynb\n",
    "def plot_explained_variance(pca):\n",
    "    plotly.offline.init_notebook_mode() \n",
    "    \n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cum_var_exp = np.cumsum(explained_var)\n",
    "    \n",
    "    plotly.offline.iplot({\n",
    "        \"data\": [Bar(y=explained_var, name='individual explained variance'),\n",
    "                 Scatter(y=cum_var_exp, name='cumulative explained variance')\n",
    "            ],\n",
    "        \"layout\": Layout(xaxis=XAxis(title='Principal components'), yaxis=YAxis(title='Explained variance ratio'))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_explained_variance(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the eigenvalues of images are calculated, we can visualize them to see the reduction analysis. They maintain the same general features without the clutter. They are extremely hard to distinguish to the human eye, but that just falls into the nature of the data.Each of the pieces represents a feature of all images rather than specific to one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eigenpiece_titles = [\"eigenpiece %d\" % i for i in range(eigenpieces.shape[0])]\n",
    "plot_gallery(eigenpieces, eigenpiece_titles,h,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear dimensionality reduction --- Randomized PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing the same methodology of full PCA, we can create a random PCA using 2667 randomized components. Again, the resulting images are very vague due to the nature of the data, but the features are still emphasized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do some PCA of the features and go from 2667 features to 5 features\n",
    "n_components = 2667\n",
    "print (\"Extracting the top %d eigenpieces from %d lego pieces\" % (\n",
    "    n_components, X.shape[0]))\n",
    "\n",
    "rpca = PCA(n_components=n_components, svd_solver='randomized')\n",
    "%time rpca.fit(X.copy())\n",
    "eigenpieces = rpca.components_.reshape((n_components, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenpiece_titles = [\"eigenpiece %d\" % i for i in range(eigenpieces.shape[0])]\n",
    "plot_gallery(eigenpieces, eigenpiece_titles,h,w,1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot that is formed shows that most of the variance can be explained by the principal component at around 30% variance. The 2-5 components also adds roughly about 30% of the variance. From there the remaining components, the remaining 35% are accumulated, and they trail off exponentially. Therefore, it adheres to the same pattern as before.</br>\n",
    "We reach an exponential variance level of 85%, which is slightly below what we developed above. Again, the most ideal level of 90% occurs at roughly 100 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_explained_variance(rpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Full vs Randomized PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to compare full PCA and Randomized PCA. Below I reconstructed an image utilizing each of the two methods. We can see that the full PCA creates a sharper images than the randomized PCA. Most likely, this is due to the full PCA having more consequtive components to create features from. However, I did find that it was more convenient to use the randomized PCA because the method was able to run much more efficiently due to the lower number of calculations needed. We can see from below, the image is still recognizable without sacrificing efficiency. However, ultimately both plots appear to require the same number of components to produce a sufficiently accurate model, so they are equally adequate for this particular run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Images Reconstruction of two types to compare\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.axis('off')\n",
    "\n",
    "for index_to_reconstruct in [10, 50, 80, 100] :\n",
    "    \n",
    "    plt.figure(figsize=(15,7))\n",
    "    \n",
    "    #Original\n",
    "    des = original_images[index_to_reconstruct].reshape(1, -1)\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(original_images[index_to_reconstruct].reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Original')\n",
    "    plt.grid()\n",
    "\n",
    "    #Full PCA\n",
    "    reconstructed_image = pca.inverse_transform(pca.transform(des))\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(reconstructed_image.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Full PCA')\n",
    "    plt.grid()\n",
    "\n",
    "    #Randomized PCA\n",
    "    reconstructed_image_rpca = rpca.inverse_transform(rpca.transform(des))\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(reconstructed_image_rpca.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Randomized PCA')\n",
    "    plt.grid()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daisy Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform a feature extraction, I chose to use DAISY. DAISY makes the image convolved with Gausian kernel function. It then calculates the projection of the gradient of each pixel by making each feature a vector with each element being the gradient. To test, I chose a step of 20 and a radius of 20 to create overlap so the feature extraction would get a better . Below, we were able to extract 5580 features from our test image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import daisy\n",
    "\n",
    "idx_to_reconstruct = 21\n",
    "img  = original_images[idx_to_reconstruct].reshape((h,w))\n",
    "features, img_desc = daisy(img, step=20, radius=20, rings=2, histograms=8, orientations=8, visualize=True)\n",
    "plt.imshow(img_desc)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = daisy(img, step=20, radius=20, rings=2, histograms=8, orientations=4, visualize=False)\n",
    "print(features.shape)\n",
    "print(features.shape[0]*features.shape[1]*features.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then took the above feature test and applied to to all the images in my 1D array of images. We were able to determine 5508 features from this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to tak in the row of the matric and return a new feature\n",
    "def apply_daisy(row,shape):\n",
    "    feat = daisy(row.reshape(shape), step=20, radius=20, \n",
    "                 rings=2, histograms=8, orientations=4, \n",
    "                 visualize=False)\n",
    "    return feat.reshape((-1))\n",
    "\n",
    "%time test_feature = apply_daisy(original_images[idx_to_reconstruct],(h,w))\n",
    "test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to entire data, row by row\n",
    "daisy_features = np.apply_along_axis(apply_daisy, 1, X, (h,w))\n",
    "print(daisy_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest Neighbor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed said feature analysis, we must now determine it's effectiveness. The first step was to see if it could determine what image was closest to a given image. From the below code, I was able to display the original image and what Daisy determined to be the closest image. As we can see, it gave us an image of a completely other brick, which would cause incorrect classification. Because of this, the DAISY method does not seem entirely helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# find closest image to current image\n",
    "idx1 = 5\n",
    "distances = copy.deepcopy(dist_matrix[idx1,:])\n",
    "distances[idx1] = np.infty # dont pick the same image!\n",
    "idx2 = np.argmin(distances)\n",
    "\n",
    "plt.figure(figsize=(7,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(original_images[idx1].reshape((h,w)))\n",
    "plt.title(\"Original Image\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(original_images[idx2].reshape((h,w)))\n",
    "plt.title(\"Closest Image\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Heatmap of Pairwise Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first created a heatmap based on the pairwise distances of the daisy features, but there was too much data and the information was impossible to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix_daisy = pairwise_distances(daisy_features)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(dist_matrix_daisy, cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the heatmap more understandable, we first break everything up into it's categories, and then apply the daisy algorithm from there. We take the mean value of each column of each matrix of bricks, and then drawing the pair-wise difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idic = {}\n",
    "\n",
    "for i in range(len(X)):\n",
    "    ci = df['Label'][i]\n",
    "    tmparray = []\n",
    "    li = idic.get(ci, tmparray)\n",
    "    tmprow = X.iloc[i]\n",
    "    li.append(tmprow)\n",
    "    idic[ci] = li\n",
    "print(idic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmparr = []\n",
    "\n",
    "for i in idic.keys():\n",
    "    cur_feature = np.mean(np.apply_along_axis(apply_daisy, 1, idic.get(i), (220,220)), axis=0).reshape((1,5508)).ravel()\n",
    "    tmparr.append(cur_feature)\n",
    "tmparr = np.asarray(tmparr)\n",
    "tmparr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix_daisy = pairwise_distances(tmparr)\n",
    "dist_matrix_daisy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resultant plot below shows that there is not big different from one brick to another. That means that there are varying differences with no particular pattern. If the block is red than there is some notable differnce in that type of brick. Therefore, there is some information in the pixel that we can extract for images. Therefore, there is a chance that we could extact the categories, but due to the lack of notability in most of the bricks, it does not seem likely the feature detection via DAISY will be accurate enough to serve our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "#plt.axis('off')\n",
    "\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "#sns.set(style=\"darkgrid\")\n",
    "#f, ax = plt.subplots(figsize=(50, 50))\n",
    "sns.heatmap(dist_matrix_daisy, cmap=cmap)\n",
    "#f.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
