{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Five: Wide and Deep Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By : Katie Rink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set : https://www.kaggle.com/datasets/whenamancodes/infoseccyber-security-salaries?select=Cyber_salaries.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 08:47:28.657131: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1349, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics as mt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "#Loading the dataset\n",
    "df = pd.read_csv('../Data/Cyber_salaries.csv', low_memory=False)\n",
    "\n",
    "#Showing data\n",
    "#df.info()\n",
    "#df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['salary_in_usd'].max()\n",
    "df['salary_in_usd'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all variables that we will not be using, so that we have a set of defined class variables. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select which variables to use\n",
    "df.drop(['work_year', 'salary', 'salary_currency', 'employee_residence'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data by removing all null values so that our data is easy to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN</td>\n",
       "      <td>FT</td>\n",
       "      <td>Information Security Officer</td>\n",
       "      <td>72762</td>\n",
       "      <td>100</td>\n",
       "      <td>DE</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Security Officer</td>\n",
       "      <td>123400</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Security Officer</td>\n",
       "      <td>88100</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Security Engineer</td>\n",
       "      <td>163575</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Security Engineer</td>\n",
       "      <td>115800</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experience_level employment_type                     job_title  \\\n",
       "0               EN              FT  Information Security Officer   \n",
       "1               SE              FT              Security Officer   \n",
       "2               SE              FT              Security Officer   \n",
       "3               SE              FT             Security Engineer   \n",
       "4               SE              FT             Security Engineer   \n",
       "\n",
       "   salary_in_usd  remote_ratio company_location company_size  \n",
       "0          72762           100               DE            S  \n",
       "1         123400             0               US            M  \n",
       "2          88100             0               US            M  \n",
       "3         163575           100               US            M  \n",
       "4         115800           100               US            M  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of rows with any missing data\n",
    "df.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace salary with categories so that it is more broad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(row):\n",
    "    k_amount = int(row['salary_in_usd'] / 10000)\n",
    "    k_amount = str(k_amount)\n",
    "    val = k_amount[0]\n",
    "    for i in range(1, len(str(k_amount))) :\n",
    "        val += '0'\n",
    "    val += 'k'\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we begin to preprocess the data by encoding categorical data as integers first <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "      <th>salary_class</th>\n",
       "      <th>experience_level_int</th>\n",
       "      <th>employment_type_int</th>\n",
       "      <th>job_title_int</th>\n",
       "      <th>company_location_int</th>\n",
       "      <th>company_size_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN</td>\n",
       "      <td>FT</td>\n",
       "      <td>Information Security Officer</td>\n",
       "      <td>72762</td>\n",
       "      <td>100</td>\n",
       "      <td>DE</td>\n",
       "      <td>S</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Security Officer</td>\n",
       "      <td>123400</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Security Officer</td>\n",
       "      <td>88100</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Security Engineer</td>\n",
       "      <td>163575</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Security Engineer</td>\n",
       "      <td>115800</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experience_level employment_type                     job_title  \\\n",
       "0               EN              FT  Information Security Officer   \n",
       "1               SE              FT              Security Officer   \n",
       "2               SE              FT              Security Officer   \n",
       "3               SE              FT             Security Engineer   \n",
       "4               SE              FT             Security Engineer   \n",
       "\n",
       "   salary_in_usd  remote_ratio company_location company_size  salary_class  \\\n",
       "0          72762           100               DE            S            13   \n",
       "1         123400             0               US            M             1   \n",
       "2          88100             0               US            M            15   \n",
       "3         163575           100               US            M             1   \n",
       "4         115800           100               US            M             1   \n",
       "\n",
       "   experience_level_int  employment_type_int  job_title_int  \\\n",
       "0                     0                    2             47   \n",
       "1                     3                    2             71   \n",
       "2                     3                    2             71   \n",
       "3                     3                    2             68   \n",
       "4                     3                    2             68   \n",
       "\n",
       "   company_location_int  company_size_int  \n",
       "0                    17                 2  \n",
       "1                    54                 1  \n",
       "2                    54                 1  \n",
       "3                    54                 1  \n",
       "4                    54                 1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Convert the salary to categories \n",
    "df['salary_class'] = df.apply(classify, axis=1)\n",
    "\n",
    "#Encode categorical data as integers  \n",
    "encoders = dict() # save each encoder in dictionary\n",
    "categorical_headers = ['experience_level','employment_type','job_title', 'company_location', 'company_size', 'salary_class']\n",
    "\n",
    "for col in categorical_headers:\n",
    "    df[col] = df[col].str.strip()\n",
    "    df[col] = df[col].str.strip()\n",
    "    if col == \"salary_class\":\n",
    "        # special case the target, just replace the column\n",
    "        tmp = LabelEncoder()\n",
    "        df[col] = tmp.fit_transform(df[col])\n",
    "    else : \n",
    "        # integer encode strings that are features\n",
    "        encoders[col] = LabelEncoder() # save the encoder\n",
    "        df[col+'_int'] = encoders[col].fit_transform(df[col])\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process the data by utilizing dimensionality reduction, scaling, etc <br/>\n",
    "**TO-DO : handle scaling with categorical values ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the numeric, continuous variables\n",
    "numeric_headers = [\"remote_ratio\"]\n",
    "\n",
    "ss = StandardScaler()\n",
    "df[numeric_headers] = ss.fit_transform(df[numeric_headers].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print final data as it is represented <br/>\n",
    "**TO-DO : describe more in depth what each variable represents and how it will affect final regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the following 6 features:\n",
      "[   'experience_level_int',\n",
      "    'employment_type_int',\n",
      "    'job_title_int',\n",
      "    'company_location_int',\n",
      "    'company_size_int',\n",
      "    'remote_ratio']\n",
      "\n",
      "Numeric Headers:\n",
      "['remote_ratio']\n",
      "\n",
      "Categorical String Headers:\n",
      "[   'experience_level',\n",
      "    'employment_type',\n",
      "    'job_title',\n",
      "    'company_location',\n",
      "    'company_size',\n",
      "    'salary_class']\n",
      "\n",
      "Categorical Headers, Encoded as Integer:\n",
      "[   'experience_level_int',\n",
      "    'employment_type_int',\n",
      "    'job_title_int',\n",
      "    'company_location_int',\n",
      "    'company_size_int']\n"
     ]
    }
   ],
   "source": [
    "categorical_headers_ints = [x+'_int' for x in categorical_headers[:-1]]\n",
    "feature_columns = categorical_headers_ints+numeric_headers\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "print(f\"We will use the following {len(feature_columns)} features:\")\n",
    "pp.pprint(feature_columns)\n",
    "\n",
    "print(\"\\nNumeric Headers:\")\n",
    "pp.pprint(numeric_headers) # normalized numeric data\n",
    "print(\"\\nCategorical String Headers:\")\n",
    "pp.pprint(categorical_headers) # string data\n",
    "print(\"\\nCategorical Headers, Encoded as Integer:\")\n",
    "pp.pprint(categorical_headers_ints) # string data encoded as an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to represent all of the features we have available to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experience_level has 4 unique values:\n",
      "['EN' 'SE' 'MI' 'EX']\n",
      "employment_type has 4 unique values:\n",
      "['FT' 'PT' 'CT' 'FL']\n",
      "job_title has 87 unique values:\n",
      "['Information Security Officer' 'Security Officer' 'Security Engineer'\n",
      " 'Penetration Testing Engineer' 'Security Analyst' 'Security Consultant'\n",
      " 'Network Security Engineer' 'Penetration Tester' 'DevSecOps Engineer'\n",
      " 'Security Specialist' 'Cloud Security Engineer'\n",
      " 'Security Operations Engineer' 'Head of Information Security'\n",
      " 'Chief Information Security Officer' 'Cyber Security Analyst'\n",
      " 'Information Security Manager' 'Network and Security Engineer'\n",
      " 'Threat Hunter' 'Information Security Compliance Lead'\n",
      " 'Digital Forensics Analyst' 'Information Security Compliance Analyst'\n",
      " 'Cyber Threat Analyst' 'Cyber Security Consultant' 'IT Security Engineer'\n",
      " 'Cyber Program Manager' 'IT Security Analyst'\n",
      " 'Application Security Architect' 'Security Researcher'\n",
      " 'Information Security Compliance Manager'\n",
      " 'Application Security Specialist' 'Security Incident Response Engineer'\n",
      " 'Ethical Hacker' 'IT Security Manager' 'Application Security Engineer'\n",
      " 'Vulnerability Analyst' 'Cyber Security Engineer'\n",
      " 'Information Security Analyst' 'Principal Application Security Engineer'\n",
      " 'Cyber Security Architect' 'SOC Analyst'\n",
      " 'Cyber Threat Intelligence Analyst' 'Information Security Specialist'\n",
      " 'Director of Information Security' 'Threat Intelligence Analyst'\n",
      " 'Cloud Security Engineering Manager' 'Application Security Analyst'\n",
      " 'Data Security Analyst' 'Detection Engineer'\n",
      " 'Principal Security Engineer' 'Information Systems Security Engineer'\n",
      " 'Staff Application Security Engineer' 'Information Security Engineer'\n",
      " 'Vulnerability Management Engineer' 'Azure Security Engineer'\n",
      " 'Security Operations Analyst' 'DevOps Security Engineer'\n",
      " 'Security DevOps Engineer' 'Vulnerability Researcher'\n",
      " 'Computer Forensic Software Engineer' 'Incident Response Analyst'\n",
      " 'Cloud Security Architect' 'Information Security Architect'\n",
      " 'Product Security Engineer' 'Lead Security Engineer'\n",
      " 'Incident Response Manager' 'Cyber Security Specialist'\n",
      " 'Cyber Security Researcher' 'Infrastructure Security Engineer'\n",
      " 'Head of Security' 'Threat Hunting Lead' 'Incident Response Lead'\n",
      " 'Corporate Security Engineer' 'Security Engineering Manager'\n",
      " 'Staff Security Engineer' 'Threat Intelligence Response Analyst'\n",
      " 'Offensive Security Engineer' 'Enterprise Security Engineer'\n",
      " 'Privacy Manager' 'Software Security Engineer'\n",
      " 'Cyber Security Training Specialist' 'IAM Engineer'\n",
      " 'Principal Cloud Security Engineer' 'Lead Information Security Engineer'\n",
      " 'Corporate Infrastructure Security Engineer' 'Security Officer 3'\n",
      " 'Lead Application Security Engineer' 'Concierge Security Engineer']\n",
      "company_location has 57 unique values:\n",
      "['DE' 'US' 'CY' 'BA' 'GB' 'CA' 'ES' 'BR' 'BW' 'SG' 'NL' 'IN' 'AE' 'CH'\n",
      " 'DK' 'CL' 'AU' 'FR' 'IT' 'JP' 'GR' 'AZ' 'RO' 'DZ' 'AQ' 'AX' 'SI' 'HR'\n",
      " 'SE' 'HU' 'ET' 'MX' 'IL' 'IE' 'PK' 'NO' 'PL' 'PT' 'RU' 'CZ' 'ID' 'EE'\n",
      " 'KE' 'RS' 'AR' 'NZ' 'BE' 'ZA' 'AT' 'UM' 'LU' 'EG' 'TW' 'VN' 'SA' 'AF'\n",
      " 'TR']\n",
      "company_size has 3 unique values:\n",
      "['S' 'M' 'L']\n",
      "salary_class has 17 unique values:\n",
      "[13  1 15  8  3 12  2 16 10  4  5  0  6  7 14  9 11]\n"
     ]
    }
   ],
   "source": [
    "# sandbox for looking at different categorical variables\n",
    "for col in categorical_headers:\n",
    "    vals = df[col].unique()\n",
    "    print(col,'has', len(vals), 'unique values:')\n",
    "    print(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our values, we created cross columns and encoded our cross columns as integers. <br/>\n",
    "**TO-DO : Determine what values to cross and justify**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experience_level_employment_type']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose these as a class, what makes sense??\n",
    "cross_columns = [\n",
    "                    ['experience_level','employment_type'],\n",
    "                ]\n",
    "\n",
    "# cross each set of columns in the list above\n",
    "cross_col_df_names = []\n",
    "for cols_list in cross_columns:\n",
    "    # encode as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    # 1. create crossed labels by join operation\n",
    "    X_crossed = df[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "    \n",
    "    # get a nice name for this new crossed column\n",
    "    cross_col_name = '_'.join(cols_list)\n",
    "    \n",
    "    # 2. encode as integers, stacking all possibilities\n",
    "    enc.fit(np.hstack((X_crossed.to_numpy())))\n",
    "    \n",
    "    # 3. Save into dataframe with new name\n",
    "    df[cross_col_name] = enc.transform(X_crossed)\n",
    "    \n",
    "    # Save the encoder used here for later:\n",
    "    encoders[cross_col_name] = enc\n",
    "    \n",
    "    # keep track of the new names of the crossed columns\n",
    "    cross_col_df_names.append(cross_col_name) \n",
    "    feature_columns.append(cross_col_name)\n",
    "    \n",
    "cross_col_df_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO : Choose metrics and describe why they are appropriate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our processing is complete, we can begin modeling. The final step to do is to split our data into testing and training. <br/>\n",
    "**TO-DO : Determine method to divide into training and testing a justify**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# combine the features into a single large matrix\n",
    "X = df[feature_columns]\n",
    "y = df['salary_class'].values.astype(np.int32)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.4, random_state=42)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our split, we can run a cross-validation on it to estimate how accurate our model will be based around our testing-training split <br/>\n",
    "**TO-DO : Select a cross validation and argue why it is a realistic mirroring**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide and Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create our first wide and deep network. In this network we utilized . <br/>\n",
    "Once we have completed the modeling, I graphed the performance of the network on the training and validation data utilizing the predetermined metrics vs each training iteration. <br/>\n",
    "**TO-DO : Create First Wide and Deep Network** <br/>\n",
    "**TO-DO : Graph Performance** <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/krink/.conda/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "2022-11-15 08:47:51.233400: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 08:47:51.235440: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 117\u001b[0m\n\u001b[1;32m    109\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit([X_train_crossed,X_train_cat,X_train_num],\n\u001b[1;32m    110\u001b[0m                 y_train_fold, \n\u001b[1;32m    111\u001b[0m                 epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, \n\u001b[1;32m    112\u001b[0m                 batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[1;32m    113\u001b[0m                 verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \n\u001b[1;32m    114\u001b[0m                 validation_data \u001b[38;5;241m=\u001b[39m ([X_test_crossed,X_test_cat,X_test_num],y_test_fold))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])) : \n\u001b[0;32m--> 117\u001b[0m     \u001b[43maccuracies\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mappend(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    118\u001b[0m     accuracies_val[index]\u001b[38;5;241m.\u001b[39mappend(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy_val\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    119\u001b[0m     losses[index]\u001b[38;5;241m.\u001b[39mappend(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow import keras\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "accuracies = []\n",
    "accuracies_val = []\n",
    "losses = []\n",
    "losses_val = []\n",
    "for i in range(splits):\n",
    "    inner = []\n",
    "    for j in range(epcohs):\n",
    "        inner.append(0)\n",
    "    losses_val.append(inner)\n",
    "    losses.append(inner)\n",
    "    accuracies_val.append(inner)\n",
    "    accuracies.append(inner)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(X, y) : \n",
    "    x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "    \n",
    "    # get crossed columns\n",
    "    X_train_crossed = x_train_fold[cross_col_df_names].to_numpy()\n",
    "    X_test_crossed = x_test_fold[cross_col_df_names].to_numpy()\n",
    "\n",
    "    # save categorical features\n",
    "    X_train_cat = x_train_fold[categorical_headers_ints].to_numpy() \n",
    "    X_test_cat = x_test_fold[categorical_headers_ints].to_numpy() \n",
    "\n",
    "    # and save off the numeric features\n",
    "    X_train_num =  x_train_fold[numeric_headers].to_numpy()\n",
    "    X_test_num = x_test_fold[numeric_headers].to_numpy()\n",
    "\n",
    "\n",
    "    # we need to create separate lists for each branch\n",
    "    crossed_outputs = []\n",
    "\n",
    "    # CROSSED DATA INPUT\n",
    "    input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "    for idx,col in enumerate(cross_col_df_names):\n",
    "    \n",
    "        # track what the maximum integer value will be for this variable\n",
    "        # which is the same as the number of categories\n",
    "        N = max(df_train[col].max(),df_test[col].max())+1\n",
    "        N = len(encoders[col].classes_)\n",
    "        N_reduced = int(np.sqrt(N))\n",
    "    \n",
    "    \n",
    "        # this line of code does this: input_branch[:,idx]\n",
    "        x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "        # now use an embedding to deal with integers as if they were one hot encoded\n",
    "        x = Embedding(input_dim=N, \n",
    "                  output_dim=N_reduced, \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "        # save these outputs to concatenate later\n",
    "        crossed_outputs.append(x)\n",
    "    \n",
    "\n",
    "    # now concatenate the outputs and add a fully connected layer\n",
    "    wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "    # reset this input branch\n",
    "    all_deep_branch_outputs = []\n",
    "    # CATEGORICAL DATA INPUT\n",
    "    input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "    for idx,col in enumerate(categorical_headers_ints):\n",
    "    \n",
    "        # track what the maximum integer value will be for this variable\n",
    "        # which is the same as the number of categories\n",
    "        N = max(df_train[col].max(),df_test[col].max())+1\n",
    "        N_reduced = int(np.sqrt(N))\n",
    "    \n",
    "        # this line of code does this: input_branch[:,idx]\n",
    "        x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "        # now use an embedding to deal with integers as if they were one hot encoded\n",
    "        x = Embedding(input_dim=N, \n",
    "                  output_dim=N_reduced, \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "        # save these outputs to concatenate later\n",
    "        all_deep_branch_outputs.append(x)\n",
    "    \n",
    "    # NUMERIC DATA INPUT\n",
    "    # create dense input branch for numeric\n",
    "    input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "    x_dense = Dense(units=20, activation='relu',name='num_1')(input_num)\n",
    "    \n",
    "    all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "\n",
    "    # merge the deep branches together\n",
    "    deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "    deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "    deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "    deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "    \n",
    "    # merge the deep and wide branch\n",
    "    final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "    final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "    model = Model(inputs=[input_crossed,input_cat,input_num], \n",
    "              outputs=final_branch)\n",
    "\n",
    "    model.compile(optimizer='adagrad',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # lets also add the history variable to see how we are doing\n",
    "    # and lets add a validation set to keep track of our progress\n",
    "    history = model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train_fold, \n",
    "                    epochs=50, \n",
    "                    batch_size=32, \n",
    "                    verbose=0, \n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test_fold))\n",
    "    \n",
    "    for index in range(len(history.history['accuracy'])) : \n",
    "        accuracies[index].append(history.history['accuracy'])\n",
    "        accuracies_val[index].append(history.history['accuracy_val'])\n",
    "        losses[index].append(history.history['loss'])\n",
    "        losses_val[index].append(history.history['loss_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(accuracies)\n",
    "\n",
    "plt.ylabel('Accuracy %')\n",
    "plt.title('Training')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(accuracies_val)\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(losses)\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(losses_val)\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create our second wide and deep network. In this network we utilized . <br/>\n",
    "Once we have completed the modeling, I graphed the performance of the network on the training and validation data vs each training iteration. <br/>\n",
    "**TO-DO : Create Second Wide and Deep Network** <br/>\n",
    "**TO-DO : Graph Performance** <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our third wide and deep network. In this network we utilized . <br/>\n",
    "Once we have completed the modeling, I graphed the performance of the network on the training and validation data vs each training iteration. <br/>\n",
    "**TO-DO : Create Third Wide and Deep Network** <br/>\n",
    "**TO-DO : Graph Performance** <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all our our different models, we can compare the performance. <br/>\n",
    "**TO-DO : Compare the performance of each of the networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Performance Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyze our models, we recreated our model with different numbers of layers. Once we have these models, we ran a cross-validation to see the affects of the layers. Then I pulled out the metrics I already determined were significant. <br/>\n",
    "**TO-DO : Create a model for each layer.** <br/>\n",
    "**TO-DO : Run a cross-validation on each model** <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have run all of our measures, we can now evaluate what the affect of each layer is. <br/>\n",
    "**TO-DO : Graph our validation metric vs each layer**\n",
    "**TO-DO : Write an analysis on what we see**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Performance Analysis\n",
    "[1 points] Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP). Alternatively, you can compare to a network without the wide branch (i.e., just the deep network). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a functioning deep and wide network, we can compare it's performance to the Multi-Layer Perceptron we were working with. In order to compare we must run both our best performing model and a standard MLP. <br/>\n",
    "**TO-DO : Create a multi-layer perceptron and run it** <br/>\n",
    "**TO-DO : Create our best wide and deep model and run it** <br/>\n",
    "**TO-DO : Graph performance metric at each iteration for each model** <br/>\n",
    "**TO-DO : Analyze the difference between each model** <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 tensorflow",
   "language": "python",
   "name": "condaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
